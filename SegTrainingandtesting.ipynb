{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SegTrainingandtesting.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvMLEURon3nO"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Sep 20 09:18:43 2020\n",
        "\n",
        "@author: Abdul Qayyum\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import cv2\n",
        "#from Code.utils.onehot import onehot\n",
        "\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "\n",
        "def onehot(data, n):\n",
        "    \"\"\"onehot ecoder\"\"\"\n",
        "    buf = np.zeros(data.shape + (n,))\n",
        "    nmsk = np.arange(data.size) * n + data.ravel()\n",
        "    buf.ravel()[nmsk - 1] = 1\n",
        "    return buf\n",
        "def convert_to_one_hot(volume, class_number):\n",
        "    '''\n",
        "    one hot??\n",
        "    :param volume: label\n",
        "    :param C: class number\n",
        "    :return:\n",
        "    '''\n",
        "    shape = [class_number]+list(volume.shape)\n",
        "    volume_one = np.eye(class_number)[volume.reshape(-1)].T\n",
        "    volume_one = volume_one.reshape(shape)\n",
        "    return volume_one\n",
        "\n",
        "#########################################################\n",
        "########################################################## dataset loader ######################################\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import cv2\n",
        "#import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset as BaseDataset\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import cv2\n",
        "\n",
        "class Dataset(BaseDataset):\n",
        "    \"\"\"Emidec Dataset. Read images, apply augmentation and preprocessing transformations.\n",
        "    \n",
        "    Args:\n",
        "        images_dir (str): path to images folder\n",
        "        masks_dir (str): path to segmentation masks folder\n",
        "        class_values (list): values of classes to extract from segmentation mask\n",
        "        augmentation (albumentations.Compose): data transfromation pipeline \n",
        "            (e.g. flip, scale, etc.)\n",
        "        preprocessing (albumentations.Compose): data preprocessing \n",
        "            (e.g. noralization, shape manipulation, etc.)\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    CLASSES = ['myocardium', 'myocardiuminfec','bloodpool','noreflow','unlabelled']\n",
        "    \n",
        "    def __init__(\n",
        "            self, \n",
        "            images_dir, \n",
        "            masks_dir, \n",
        "            classes=None, \n",
        "            augmentation=None, \n",
        "            preprocessing=None,\n",
        "            transform=None\n",
        "    ):\n",
        "        self.ids = os.listdir(images_dir)\n",
        "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
        "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
        "        self.transform=transform\n",
        "        \n",
        "        # convert str names to class values on masks\n",
        "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
        "        \n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing = preprocessing\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "        # read data\n",
        "        image = cv2.imread(self.images_fps[i])\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image_name = self.ids[i]\n",
        "        #mask = cv2.imread(self.masks_fps[i], 0)\n",
        "        mask = cv2.imread(self.masks_fps[i], 0)\n",
        "        \n",
        "        # extract certain classes from mask (e.g. cars)\n",
        "        masks = [(mask == v) for v in self.class_values]\n",
        "        mask = np.stack(masks, axis=-1).astype('float')\n",
        "        # add background if mask is not binary\n",
        "        if mask.shape[-1] != 1:\n",
        "            background = 1 - mask.sum(axis=-1, keepdims=True)\n",
        "            mask = np.concatenate((mask, background), axis=-1)\n",
        "            mask=mask.transpose(2,0,1) # n_class*w*H\n",
        "            mask=torch.FloatTensor(mask)\n",
        "            #mask=mask.transpose(2,0,1) # n_class*w*H\n",
        "        #onehot_label=torch.FloatTensor(img_label_onehot)\n",
        "        #print(onehot_label.shape)\n",
        "            \n",
        "        \n",
        "        # apply augmentations\n",
        "        if self.augmentation:\n",
        "            sample = self.augmentation(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "        \n",
        "        # apply preprocessing\n",
        "        if self.preprocessing:\n",
        "            sample = self.preprocessing(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "        \n",
        "        if self.transform:\n",
        "            image=self.transform(image)\n",
        "            \n",
        "        return image, mask,image_name\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "    \n",
        "imagepath='/raid/Home/Users/aqayyum/pymultimodel/Covid19Segmentationmodels/StructSegdataset/emidecdataset/Trainimagenew/'\n",
        "maskpath='/raid/Home/Users/aqayyum/pymultimodel/Covid19Segmentationmodels/StructSegdataset/emidecdataset/Trainmasksequence/'    \n",
        "x_train_dir=imagepath\n",
        "y_train_dir=maskpath\n",
        "#data=data[:,:,0]  \n",
        "# Lets look at data we have\n",
        "from torchvision import transforms\n",
        "transform=transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize(mean=[0.485,0.456,0.406],\n",
        "                                                  std=[0.229,0.224,0.225])])\n",
        "#dataset = Dataset(x_train_dir, y_train_dir, classes=['leftlung', 'rightlung', 'disease'],transform=transform)\n",
        "#dataset = Dataset(x_train_dir, y_train_dir, classes=['leftlung', 'rightlung', 'disease'],transform=transform)\n",
        "\n",
        "#image, mask = dataset[4] # get some sample\n",
        "#print(image.shape)\n",
        "#print(mask.shape)\n",
        "#batch_size=1\n",
        "from torch.utils.data import DataLoader\n",
        "#train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "#for train_batch in train_dataloader:\n",
        "#    print(train_batch[0].shape)\n",
        "#    print(train_batch[1].shape)\n",
        "#    break\n",
        "#transform=transforms.Compose([transforms.ToTensor(),\n",
        "#                              transforms.Normalize(mean=[0.485,0.456,0.406],\n",
        "#                                                  std=[0.229,0.224,0.225])])\n",
        "\n",
        "#imagepath='/raid/Home/Users/aqayyum/pymultimodel/Covid19Segmentationmodels/Imgs/'\n",
        "#maskpath='/raid/Home/Users/aqayyum/pymultimodel/Covid19Segmentationmodels/GT/'\n",
        "\n",
        "\n",
        "#train_dataset=Mydataset(imagepath,maskpath,transform)\n",
        "\n",
        "# batch_size=4\n",
        "# from torch.utils.data import DataLoader\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "# for train_batch in train_dataloader:\n",
        "#     print(train_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z56QUjxWoKZR"
      },
      "source": [
        "#################################################################segmentation model resnet ################################\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class double_conv(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2'''\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(double_conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "            \n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class one_conv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(one_conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_ch, out_ch, 1)    \n",
        "        )\n",
        "    def forward(self,x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class inconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(inconv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(down, self).__init__()\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.mpconv = double_conv(in_ch, out_ch)\n",
        "        self.bridge = one_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(x)\n",
        "        #print(x.shape)\n",
        "        x_1 = self.mpconv(x)\n",
        "        #print(x_1.shape)\n",
        "        x_2 = self.bridge(x)\n",
        "        #print(x_2.shape)\n",
        "        x = x_1 + x_2\n",
        "        return x\n",
        "\n",
        "\n",
        "class up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
        "        super(up, self).__init__()\n",
        "\n",
        "        #  would be a nice idea if the upsampling could be learned too,\n",
        "        #  but my machine do not have enough memory to handle all those weights\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
        "\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "        self.bridge = one_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        diffX = x1.size()[2] - x2.size()[2]\n",
        "        diffY = x1.size()[3] - x2.size()[3]\n",
        "        x2 = F.pad(x2, (diffX // 2, int(diffX / 2),\n",
        "                        diffY // 2, int(diffY / 2)))\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        x = self.conv(x) + self.bridge(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class outconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(outconv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_ch, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class ResUNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super(ResUNet, self).__init__()\n",
        "        self.inc = inconv(n_channels, 64)\n",
        "        self.down1 = down(64, 128)\n",
        "        self.down2 = down(128, 256)\n",
        "        self.down3 = down(256, 512)\n",
        "        self.down4 = down(512, 512)\n",
        "        self.up1 = up(1024, 256)\n",
        "        self.up2 = up(512, 128)\n",
        "        self.up3 = up(256, 64)\n",
        "        self.up4 = up(128, 64)\n",
        "        self.outc = outconv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.outc(x)\n",
        "        # x = F.softmax(x, dim=1)\n",
        "        return x\n",
        "    \n",
        "modelUres=ResUNet(3,5)\n",
        "print(modelUres)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjxjk9wCoP0v"
      },
      "source": [
        "##################################################training path######################################\n",
        "import os\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "#from Code.utils.dataloader_MulClsLungInf_UNet import LungDataset\n",
        "from torchvision import transforms\n",
        "# from LungData import test_dataloader, train_dataloader  # pls change batch_size\n",
        "from torch.utils.data import DataLoader\n",
        "#from Code.model_lung_infection.InfNet_UNet import *\n",
        "\n",
        "\n",
        "def train(epo_num, n_classes, n_channels, batch_size, lr, save_path):\n",
        "    #dataset = Dataset(x_train_dir, y_train_dir, classes=['leftlung', 'rightlung', 'disease'],transform=transform)\n",
        "    dataset = Dataset(x_train_dir, y_train_dir, classes=['myocardium', 'myocardiuminfec','bloodpool','noreflow'],transform=transform)\n",
        "    \n",
        "    train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    modelUres=ResUNet(n_channels,n_classes)\n",
        "    modelUres=modelUres.to(device)\n",
        "\n",
        "    criterion = nn.BCELoss().to(device)\n",
        "    #optimizer = optim.SGD(lung_model.parameters(), lr=lr, momentum=0.7)\n",
        "    optimizer = optim.SGD(modelUres.parameters(), lr=lr, momentum=0.7)\n",
        "\n",
        "    print(\"#\" * 20, \"\\nStart Training (network-Net)\\ \"\n",
        "                    \"\\n\", \"#\" * 20)\n",
        "\n",
        "    for epo in range(epo_num):\n",
        "\n",
        "        train_loss = 0\n",
        "        #lung_model.train()\n",
        "        modelUres.train()\n",
        "\n",
        "        for index, (img, img_mask, _) in enumerate(train_dataloader):\n",
        "\n",
        "            img = img.to(device)\n",
        "            #pseudo = pseudo.to(device)\n",
        "            img_mask = img_mask.to(device)\n",
        "            #img_mask=np.transpose(img_mask, (0, 2, 1, 3))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            #utput = lung_model(img)\n",
        "            output=modelUres(img)\n",
        "\n",
        "            output = torch.sigmoid(output)  # output.shape is torch.Size([4, 2, 160, 160])\n",
        "            loss = criterion(output, img_mask)\n",
        "\n",
        "            loss.backward()\n",
        "            iter_loss = loss.item()\n",
        "            train_loss += iter_loss\n",
        "            optimizer.step()\n",
        "\n",
        "            if np.mod(index, 20) == 0:\n",
        "                print('Epoch: {}/{}, Step: {}/{}, Train loss is {}'.format(epo, epo_num, index, len(train_dataloader), iter_loss))\n",
        "\n",
        "        os.makedirs('./checkpoints//UNet_Multi-Class', exist_ok=True)\n",
        "        if np.mod(epo+1, 10) == 0:\n",
        "            torch.save(modelUres.state_dict(),\n",
        "                       './Snapshots/save_weightsheart/{}/unet_model_{}.pkl'.format(save_path, epo+1))\n",
        "            print('Saving checkpoints: unet_model_{}.pkl'.format(epo+1))\n",
        "\n",
        "train(epo_num=300,n_classes=5,n_channels=3,batch_size=16,lr=1e-2,save_path='save_trained_weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIo8BVeXoo6_"
      },
      "source": [
        "snapshot_dir='/raid/Home/Users/aqayyum/pymultimodel/Covid19Segmentationmodels/Snapshots/save_weightsheart/save_trained_weights/unet_model_100.pkl'\n",
        "from tqdm import tqdm\n",
        "num_classes=3\n",
        "import natsort\n",
        "import matplotlib.pyplot as plt\n",
        "import natsort\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import io, exposure, img_as_uint, img_as_float\n",
        "import imageio\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from skimage.transform import resize\n",
        "#from keras.models import Model\n",
        "#import nibabel as nib\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import natsort\n",
        "                \n",
        "##############################################################load trained model ############################################\n",
        "\n",
        "input_channels=3\n",
        "num_classes=5\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#lung_model = Inf_Net_UNet(input_channels, num_classes).cuda()\n",
        "modelUres =ResUNet(3,5).cuda()\n",
        "modelUres.load_state_dict(torch.load(snapshot_dir))\n",
        "modelUres.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9JmgLF6o-C_"
      },
      "source": [
        "################################################################save image as case folder orpatients #############################################\n",
        "path1='/raid/Home/Users/aqayyum/pymultimodel/Covid19Segmentationmodels/StructSegdataset/emidecdataset/testimages/'\n",
        "oslist=os.listdir(path1)\n",
        "filesnew=natsort.natsorted(oslist)\n",
        "path_savemask='/raid/Home/Users/aqayyum/pymultimodel/Covid19Segmentationmodels/ModelsPredictions/predictemidec/'\n",
        "for i, volume in enumerate(filesnew):\n",
        "    print(volume)\n",
        "    cur_path = os.path.join(path1, volume)\n",
        "    files=natsort.natsorted(os.listdir(cur_path))\n",
        "    alist=[]\n",
        "    for n, id_ in tqdm(enumerate(files), total=len(files)):\n",
        "        print(n)\n",
        "        print(id_)\n",
        "        #img=imageio.imread(os.path.join(cur_path,id_))\n",
        "        img=Image.open(os.path.join(cur_path,id_))\n",
        "        #print(img)\n",
        "        img= transform(img)\n",
        "        img=img.to(device)\n",
        "        #print(type(img1))\n",
        "        img1=img.unsqueeze(0)\n",
        "        #output=lung_model(img1)\n",
        "        output = modelUres(img1)\n",
        "        #output1 = lung_model(img1)\n",
        "        output = torch.sigmoid(output)  # output.shape is torch.Size([4, 2, 160, 160])\n",
        "        b, _, w, h = output.size()\n",
        "        print(output.size())\n",
        "        pred = output.cpu().permute(0, 2, 3, 1).contiguous().view(-1, num_classes).max(1)[1].view(b, w, h).numpy().squeeze()\n",
        "        #pred = output.cpu().contiguous().view(-1, num_classes).max(1)[1].view(b, w, h).numpy().squeeze()\n",
        "        #print('Class numbers of prediction in total:', np.unique(pred))\n",
        "        # pred = misc.imresize(pred, size=(w_gt, h_gt))\n",
        "        #os.makedirs(save_path, exist_ok=True)\n",
        "        savpath=os.path.join(path_savemask,str(volume))\n",
        "        savefold=createFolder(savpath)\n",
        "        #io.imsave(savpath+'/'+str(n)+'.png',pred)\n",
        "        cv2.imwrite(savpath+'/'+str(n)+'.png',pred)\n",
        "        alist.append(pred)  \n",
        "\n",
        "\n",
        "########################################################\n",
        "print('done or complete')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}